---
layout: default
title:  "Derivation of the Kalman Filter in a Fully Bayesian Framework with an Application to Stock Prices Forecasting"
date:   2024-07-14 18:00:00 +0200
categories: jekyll update
excerpt: "Filtering is a statistical technique used to extract information from a noisy signal. The core idea of this methodology is that the observations are produced by 
a vector of hidden states. While the observations are independent from each other, the hidden states follow a first-order Markov process (i.e. the state at time  depends only on the state at time ).
As an example consider the following: it is a pleasant day in the city park and you amuse yourself observing people walking along a fairly distant and tree-lined trail which is perfectly perpendicular 
to your position. In particular, you fix your attention on a certain person. At time  you observe him at point , then he walks behind a bush and you try to estimate where he will be at the next time s
tep. Having formed a rough idea of his velocity, you expect to see him appear at point . However, when time  finally comes, you see him at point , then he disappears again  behind a large tree. 
Leveraging on the information at the preceding step, you modify your beliefs and refine your prediction for time .
The model specifies a structure of the world where observations are produced by latent states:"
image: /assets/images/rambosson.jpg
---
<!DOCTYPE html>
<html>

<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Derivation of the Kalman Filter</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        linebreaks: { automatic: true, width: "container" },
        tags: 'ams'
      },
      chtml: {
        scale: 1,
        minScaleAdjust: 75,
        matchFontHeight: true
      },
      startup: {
        pageReady: () => {
          return MathJax.startup.defaultPageReady().then(() => {
            MathJax.startup.document.state(0);
            MathJax.startup.document.rerender();
          });
        }
      }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style type="text/css">
        body, html {
            width: 100%;
            max-width: 100%;
            margin: 0;
            padding: 0;
            font-size: 16px; /* Base font size */
            overflow-x: hidden;
        }

        .MathJax_Display, .math.display {
            max-width: 100%;
            overflow-x: auto; /* Enables horizontal scrolling */
            overflow-y: hidden; /* Disables vertical scrolling */
            display: block;
            white-space: nowrap; /* Ensures no wrapping */
        }

        .math {
            font-size: 1rem; /* Responsive font size for math formulas */
        }

        @media (max-width: 768px) {
            .MathJax_Display, .math.display {
                font-size: 0.8rem; /* Smaller font size for smaller screens */
            }
        }

        sub, sup {
            font-size: 0.75em; /* Proper scaling for subscripts and superscripts */
            line-height: 0;
            position: relative;
            vertical-align: baseline;
        }

        sub {
            bottom: -0.25em;
        }

        sup {
            top: -0.5em;
        }

        img {
            max-width: 100%;
            height: auto;
        }
    </style>
</head>

<body>
<p>Filtering is a statistical technique used to extract information from
a noisy signal. In this article I will be exploring the Kalman filter,
whose core idea is that the observations (assumed to be noisy) are
produced by a vector of hidden states.</p>
<p>While the observations are independent from each other, the hidden
states follow a first-order Markov process (i.e. the state at time <span
class="math inline">\(t\)</span> depends only on the state at time <span
class="math inline">\(t - 1\)</span>).</p>
<p>As an example consider the following: it is a pleasant day in the
city park, and you amuse yourself by observing people walking along a
fairly distant tree-lined trail, which is perfectly perpendicular to
your position. In particular, you fix your attention on a certain
person. At time <span class="math inline">\(t - 2\)</span> you observe
him at point <span class="math inline">\(p_{1}\)</span>, then he walks
behind a bush and you try to estimate where he will be at the next time
step. Having formed a rough idea of his velocity, you expect to see him
appear at point <span class="math inline">\(p_{2}\)</span>. However,
when time <span class="math inline">\(t - 1\)</span> finally comes, you
see him at point <span class="math inline">\(p_{1.5}\)</span>, then he
disappears again behind a large tree. Leveraging on the information at
the preceding step, you modify your beliefs and refine your prediction
for time <span class="math inline">\(t\)</span>.</p>
<p style="text-align:center"><img src="{{ '/assets/images/post_images/kalman_filter/kalman_filter_DAG.png' | relative_url }}" style="width:3.38452in;height:2.08227in;"
alt="kalman_filter_DAG.png" /></p>
<p>Fig. 1 Example of structural time series where a set of hidden states
<span class="math inline">\(\mathbf{x}\)</span>, partly observed or not
observed, influence the dependent variable(s) <span
class="math inline">\(\mathbf{y}\)</span>.</p>
<p>It is apparent that two main problems should be addressed: how it is
possible to infer anything about unobserved quantities if no data is
available, and how it is possible to determine the ways in which the
unobserved quantities become visible and produce certain observed
outcomes.</p>
<h2><strong>A Structural Model</strong></h2>
<p>Assume a state vector <span class="math inline">\(\mathbf{x}\)</span>
and a vector of observations <span
class="math inline">\(\mathbf{y}\)</span>, then it possible to assume a
dependency between the latent state space and the dependent variables of
the type:</p>
<p><span class="math display">\[\mathbf{x}_{t} = f(\mathbf{x}_{t -
1},\mathbf{w}_{t}) = F\mathbf{x}_{\mathbf{t} - \mathbf{1}} +
G\mathbf{w}_{\mathbf{t}}\]</span></p>
<p><span class="math display">\[\mathbf{y}_{t} =
f(\mathbf{x}_{t},\mathbf{e}_{t}) = H\mathbf{x}_{t} +
\mathbf{e}_{t}\]</span></p>
<p>The above formulation is equivalent to affirming that the state at
time <span class="math inline">\(t\)</span> depends on the state at time
of <span class="math inline">\(t - 1\)</span> (is a first-order Markov
process) through a “rule” specified by <span
class="math inline">\(F\)</span>, called “transition matrix,” which
describes how a state succeeds another. This evolution is not perfect
and might be subjected to random noise <span
class="math inline">\(\mathbf{w}_{t}\)</span>, whose associated matrix
<span class="math inline">\(G\)</span> is referred to as to “control
matrix,” and describes how external interventions perturb the
system.</p>
<p>On the other hand, the observations at time <span
class="math inline">\(t\)</span> are dependent on the latent state at
that moment, through a rule specified in the matrix <span
class="math inline">\(H\)</span>, and accounting for imperfect
transitions from the latent space to the observable space through a
random noise <span class="math inline">\(\mathbf{e}_{t}\)</span>.</p>
<p>In order to be able to derive the model in a fully Bayesian setting,
it is necessary to assume that the error terms are Gaussian with mean
<span class="math inline">\(\mathbf{0}\)</span> and variance-covariance
matrices <span class="math inline">\(\mathbf{Q}\)</span> and <span
class="math inline">\(\mathbf{R}\)</span>.</p>
<p><span class="math display">\[w_{t} \sim
N(\mathbf{0},\mathbf{Q})\]</span></p>
<p><span class="math display">\[e_{t} \sim
N(\mathbf{0},\mathbf{R})\]</span></p>
<p>It is possible to frame the problem in Bayesian terms as follows:
“find a rule that states the beliefs about state <span
class="math inline">\(\mathbf{x}_{t}\)</span> at time <span
class="math inline">\(t - 1\)</span>, then update the belief once <span
class="math inline">\(\mathbf{y}_{t}\)</span> is observed.” More
formally, this is expressed in two steps: prediction and update.</p>
<p><em><strong>prediction step</strong></em></p>
<p><span class="math display">\[p(x_{t}|y_{1:t - 1}) = \int p(x_{t},x_{t
- 1}|y_{1:t - 1})dx_{t - 1}\]</span></p>
<p><span class="math display">\[= \int p(x_{t}|x_{t - 1})p(x_{t -
1}|y_{1:t - 1})dx_{t - 1}\ \ \ \mathbf{(1)}\]</span></p>
<p>Which is known at every time step if all the values, including the
transition matrix, are arbitrarily initialised.</p>
<p><em><strong>update step</strong></em></p>
<p><span class="math display">\[p(x_{t}|y_{1:t}) \propto p(y_{t},y_{t -
1}|x_{t})p(x_{t})\ \ \ \ \ By\ Bayes\ theorem\]</span></p>
<p><mark></mark></p>
<p><span class="math display">\[\approx p(y_{t}|y_{1:t -
1},x_{t})p(y_{1:t - 1}|x_{t})p(x_{t})\]</span></p>
<p>However, by the application of Bayes Theorem, it is possible to
rewrite the last step as:<mark></mark></p>
<p><mark></mark></p>
<p><span class="math display">\[\approx p(y_{t}|x_{t})p(x_{t}|y_{1:t -
1})\ \ \mathbf{(2)}\]</span><mark></mark></p>
<p>Note that the second factor of <strong>(2)</strong> is equal to
<strong>(1)</strong> first step, therefore the recursion is
complete.</p>
<h2><strong>A Study of (2)</strong></h2>
<p>Let’s consider more in depth <strong>(2)</strong>: note that the
expression is made up of two distributions multiplied one by the other.
The formula may be interpreted as a Bayesian update step, and since by
hypothesis the noise is normally distributed, it is possible to conclude
that the resulting “posterior” should have closed form (by
conjugacy.)</p>
<p><strong>Studying</strong> <span
class="math inline">\(\mathbf{p}(\mathbf{y}_{\mathbf{t}}|\mathbf{x}_{\mathbf{t}})\)</span></p>
<p><span class="math display">\[\mathbf{E}\lbrack y_{t|t}\rbrack =
E\lbrack H\mathbf{x}_{t} + \mathbf{e}_{t}\rbrack\]</span></p>
<p><span class="math display">\[= HE\lbrack\mathbf{x}_{t}\rbrack +
E\lbrack\mathbf{e}_{t}\rbrack\]</span></p>
<p><span class="math display">\[= H\mathbf{x}_{t}\]</span></p>
<p>And since the quantities are independent:</p>
<p><span class="math display">\[V\lbrack y_{t|t}\rbrack = V\lbrack
H\mathbf{x}_{t} + \mathbf{e}_{t}\rbrack\]</span></p>
<p><span class="math display">\[= V\lbrack H\mathbf{x}_{t}\rbrack +
V\lbrack\mathbf{e}_{t}\rbrack\]</span></p>
<p>Using the hypotheses on the noise terms made above, and the
properties of variance:</p>
<p><span class="math display">\[= HV\lbrack\mathbf{x}_{t}\rbrack H^{T} +
R\]</span></p>
<p><span class="math display">\[= R\]</span></p>
<p>Since the noise is Gaussian, it is possible to conclude that:</p>
<p><span class="math display">\[p(y_{t}|x_{t}) \sim
N(H\mathbf{x}_{t},R)\]</span></p>
<p><strong>Studying</strong> <span
class="math inline">\(\mathbf{p}(\mathbf{x}_{\mathbf{t}}|\mathbf{y}_{\mathbf{1}:\mathbf{t}
- \mathbf{1}})\)</span></p>
<p><span class="math display">\[E\lbrack\mathbf{x}_{t|t - 1}\rbrack =
E\lbrack F\mathbf{x}_{\mathbf{t} - \mathbf{1}} +
G\mathbf{w}_{\mathbf{t}}\rbrack\]</span></p>
<p>At time <span class="math inline">\(t - 1\)</span> the value for
<span class="math inline">\(\mathbf{x}_{t - 1}\)</span> is just an
estimate:</p>
<p><span class="math display">\[= E\lbrack F\mathbf{x}_{\mathbf{t} -
\mathbf{1}}\rbrack + E\lbrack G\mathbf{w}_{t}\rbrack\]</span></p>
<p>For the ease of notation it is possible to indicate the above
quantity as:</p>
<p><span class="math display">\[E\lbrack\mathbf{x}_{t|t - 1}\rbrack =
{\widehat{\mathbf{x}}}_{\mathbf{t}|\mathbf{t} - \mathbf{1}} =
F{\widehat{\mathbf{x}}}_{\mathbf{t} - \mathbf{1}|\mathbf{t} -
\mathbf{1}}\]</span></p>
<p>Moreover,</p>
<p><span class="math display">\[V\lbrack\mathbf{x}_{t|t - 1}\rbrack =
V\lbrack F\mathbf{x}_{\mathbf{t} - \mathbf{1}} +
G\mathbf{w}_{\mathbf{t}}\rbrack\]</span></p>
<p><span class="math display">\[= FV\lbrack\mathbf{x}_{\mathbf{t} -
\mathbf{1}|\mathbf{t} - \mathbf{1}}\rbrack F^{T} +
GV\lbrack\mathbf{w}_{\mathbf{t}}\rbrack G^{T}\ \ \
\mathbf{(3)}\]</span></p>
<p>To make things clearer it is possible to use <span
class="math inline">\(\widehat{S}\)</span> to indicate the variance of
the hidden states, with the indices indicating what is known and what is
to be estimated.<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> Using the hypotheses introduced
above, it possible to rewrite <strong>(3)</strong> as:</p>
<p><span class="math display">\[V\lbrack\mathbf{x}_{t|t - 1}\rbrack =
{\widehat{S}}_{t|t - 1} = F{\widehat{S}}_{t - 1|t - 1}F^{T} +
GQG^{T}\]</span></p>
<p>Since the noise is Gaussian,</p>
<p><span class="math display">\[p(x_{t}|y_{1:t - 1}) \sim
N({\widehat{\mathbf{x}}}_{\mathbf{t}|\mathbf{t} -
\mathbf{1}},{\widehat{\mathbf{S}}}_{\mathbf{t}|\mathbf{t} -
\mathbf{1}})\]</span></p>
<h2><strong>Calculating the Posterior</strong></h2>
<p>The above passages have allowed the conclusion that at time <span
class="math inline">\(t\)</span>, the prediction-update steps should be
combined in the following way:</p>
<p><span class="math display">\[p(x_{t}|y_{1:t}) \propto
p(y_{t}|x_{t})p(x_{t}|y_{1:t - 1}) \approx
N(H\mathbf{x}_{t},R)N({\widehat{\mathbf{x}}}_{\mathbf{t}|\mathbf{t} -
\mathbf{1}},{\widehat{\mathbf{S}}}_{\mathbf{t}|\mathbf{t} -
\mathbf{1}})\]</span></p>
<p>Where the problem has been stated in Bayesian terms as the product of
two distributions. Since the distributions involved are both Normal, the
resulting distribution should be also Normal, and it is possible to find
a closed form for both the mean and the variance. Suppressing
momentarily the indices (they will be introduced again at the end), and
expanding the calculations:</p>
<p><span
class="math display">\[N(H\mathbf{x}_{t},R)N({\widehat{\mathbf{x}}}_{\mathbf{t}|\mathbf{t}
- \mathbf{1}},{\widehat{\mathbf{S}}}_{\mathbf{t}|\mathbf{t} -
\mathbf{1}}) \propto \exp\{ - \frac{1}{2}\lbrack y^{T}R^{- 1}y -
y^{T}R^{- 1}Hx - x^{T}H^{T}R^{- 1}y + x^{T}H^{T}R^{- 1}Hx +
x^{T}{\widehat{S}}^{- 1}x - x^{T}{\widehat{S}}^{- 1}\widehat{x} -
{\widehat{x}}^{T}{\widehat{S}}^{- 1}x +
{\widehat{x}}^{T}{\widehat{S}}^{- 1}\widehat{x}\rbrack\}\]</span></p>
<p>Using the properties of the transpose operator, collecting common
terms, and dropping those not dependent on <span
class="math inline">\(x_{t}\)</span>, it is possible to obtain:</p>
<p><span class="math display">\[\propto \exp\{ - \frac{1}{2}\lbrack -
2x^{T}(H^{T}R^{- 1}y + {\widehat{S}}^{- 1}\widehat{x}) + x^{T}(H^{T}R^{-
1}H + {\widehat{S}}^{- 1})x\rbrack\}\]</span></p>
<p>The above implies, by completing the square, that the posterior
distribution takes the form of<span
class="math inline">\(N(\mu,\mathbf{\Sigma})\)</span>, where:</p>
<p><span class="math display">\[\mathbf{\Sigma} = (H^{T}R^{- 1}y_{t} +
{\widehat{S}}_{t|t - 1}^{- 1}{\widehat{x}}_{t|t - 1})^{- 1}\]</span></p>
<p><span class="math display">\[\mu = \mathbf{\Sigma}(H^{T}R^{- 1}y_{t}
+ {\widehat{S}}_{t|t - 1}^{- 1}{\widehat{x}}_{t|t - 1})\]</span></p>
<p>Since <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\mathbf{\Sigma}\)</span> represent the mean and
value of the probability distribution of the states at time <span
class="math inline">\(t\)</span>, the recursion is complete and what
needs to be specified are only some initial values. The following
chapter will provide an applied example.</p>
<h2><strong>Forecasting Amazon Stock Prices</strong></h2>
<p>In order to put into practice what has been derived thus far,
consider a time-series of Amazon stock prices.<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
Let’s consider the adjusted closing price, i.e. the closing price
accounting for dividends, stock splits and stock offerings.</p>
<p>The price at the current time will be modelled using three states:
the price at the previous time, the velocity at which the price
increases, and its acceleration. This modelling approach has been chosen
since kinematics does not include <em>per se</em> any causal hypothesis:
it models movement in a purely geometric fashion. It is the underlying,
acyclical structure of the present model that affords a causal
interpretation. I hope that through this choice it will become easier to
understand the benefits of the current methodology.</p>
<p>Therefore, assuming that there is no external action (a simplifying
assumption that allows not to consider the market structure, political
events etc..), it is possible to ignore the matrix <span
class="math inline">\(G\)</span>. The model for the target states
is:</p>
<p><span class="math display">\[\mathbf{x}_{t} = p_{t - 1} + v_{t -
1}\Delta t + 0.5a_{t - 1}\Delta t^{2} + \mathbf{w}_{t}\ \ \ \
\mathbf{(4)}\]</span></p>
<p>With <span class="math inline">\(p\)</span> being the price, <span
class="math inline">\(v\)</span> the velocity and <span
class="math inline">\(a\)</span> the acceleration. In particular,</p>
<p><span class="math display">\[v_{t} = v_{t - 1} + a_{t - 1}\Delta_{t}
+ w_{v}\ \ \ \mathbf{(5)}\]</span></p>
<p><span class="math display">\[a_{t} = a_{t - 1} + w_{a}\ \ \
\mathbf{(6)}\]</span></p>
<p>While the above equation might seem puzzling at first, try
considering the price as the position of the stock. In that case, the
kinematic laws require that the velocity and the position at time <span
class="math inline">\(t\)</span> should be in the following relation
(ignoring noise for ease of notation):</p>
<p><span class="math display">\[v_{t} = v_{t - 1} +
\int_{0}^{t}adt\]</span></p>
<p><span class="math display">\[x_{t} = x_{t - 1} + \int_{0}^{\Delta
t}v_{t}dt\ \ \ \mathbf{(7)}\]</span></p>
<p>Expanding the calculations of <strong>(7)</strong> yields
<strong>(4)</strong>, since the integral of acceleration is the velocity
and the integral of velocity is the position (price).</p>
<p>The constant acceleration model just defined can be expressed in
matrix form as follows (note that the first matrix represents <span
class="math inline">\(F\)</span>):</p>
<p><span class="math display">\[\mathbf{x}_{t} = \begin{bmatrix}
1 &amp; \Delta t &amp; 0.5\Delta t^{2} \\
0 &amp; 1 &amp; \Delta t \\
0 &amp; 0 &amp; 1
\end{bmatrix}\left\lbrack \begin{array}{r}
p_{t - 1} \\
v_{t - 1} \\
a_{t - 1}
\end{array} \right\rbrack + \left\lbrack \begin{array}{r}
w_{p} \\
w_{v} \\
w_{a}
\end{array} \right\rbrack\]</span></p>
<p>Another element that might cause confusion is the inclusion of
quadratic terms as parameters. It should be noted that the model remains
linear in the variables, however, given the presence of a quadratic term, the 
observations cannot be construed as a linear function of the state space. Notably, the complexity of financial
markets usually requires non-linear models, and the present discussion’s
aim remains confined to a general introduction of the filtering
technique, leaving a more involved modeling to a future article
(particle filter.)</p>
<p>For the time being the acceleration term should be regarded as an approximation, it will be useful to compare it
with the more advanced models. It might be interesting to note how the model performs well in the linear part and finds some difficulties 
in the quadratic part.</p>
<p>Nonetheless, consider the structure of matrix <span class="math inline">\(H\)</span> in the application to stock prices forcasting: 
the only observed variable is the price, while velocity and acceleration get multiplied by zero. While this partly mitigates 
the problems presented thus far, it is withouth prejudice to the above criticisms and discussion.</p>
<p>Now it is necessary to model the error terms. As will be recalled, it
is assumed that they follow a Gaussian distribution. While it would be
possible to model them separately with mean <span
class="math inline">\(\mathbf{0}\)</span> and variance-covariance matrix
<span class="math inline">\(Q_{w_{i}},i \in \{ p,v,a\}\)</span>, a more
elegant approach is available: assuming that the noise in <span
class="math inline">\(w_{a}\)</span> propagates to the noise of the
other elements (as <strong>(7)</strong> would reasonably imply.)</p>
<p>Note that <span class="math inline">\(V\lbrack w_{a}\rbrack =
\sigma_{a}^{2}\)</span>, and that, in a process not dissimilar from the
one carried out above (the error could be included in
<strong>(7)</strong> but it is possible to study the integrals
separately due to the additivity of the operation):</p>
<p><span class="math display">\[w_{v} = \int_{0}^{\Delta t}w_{a}dt =
w_{a}\Delta t\]</span></p>
<p><span class="math display">\[w_{x} = \int_{0}^{\Delta
t}\int_{0}^{t}w_{a}dt = \frac{\Delta t^{2}}{2}w_{a}\]</span></p>
<p>Moreover,</p>
<p><span class="math display">\[V\lbrack w_{v}\rbrack =
\sigma_{a}^{2}\Delta t^{2}\]</span></p>
<p><span class="math display">\[V\lbrack w_{x}\rbrack = \frac{\Delta
t^{4}}{4}\sigma_{a}^{2}\]</span></p>
<p>By substituting <strong>(4)</strong>, <strong>(5)</strong>,
<strong>(6)</strong> into <span class="math inline">\(Cov(X,Y) =
E\lbrack(x - E\lbrack x\rbrack)(y - E\lbrack y\rbrack)\rbrack\)</span>,
it is possible to obtain:</p>
<p><span class="math display">\[Cov(x,v) = \frac{\Delta
t^{3}}{2}\sigma_{a}^{2}\]</span></p>
<p><span class="math display">\[Cov(x,a) = \frac{\Delta
t^{2}}{2}\sigma_{a}^{2}\]</span></p>
<p><span class="math display">\[Cov(x,a) = \Delta
t\sigma_{a}^{2}\]</span></p>
<p>Expressing all the calculations in compact form as the components of
a “noise propagation” matrix:</p>
<p><span class="math display">\[Q = \sigma_{a}^{2}\begin{bmatrix}
\frac{\Delta t^{4}}{4} &amp; \frac{\Delta t^{3}}{2} &amp; \frac{\Delta
t^{2}}{2} \\
\frac{\Delta t^{3}}{2} &amp; \Delta t^{2} &amp; \Delta t \\
\frac{\Delta t^{2}}{2} &amp; \Delta t &amp; 1
\end{bmatrix}\]</span></p>
<p>Now let’s turn to the observation model:</p>
<p><span class="math display">\[y_{t} = H\mathbf{x}_{t} + R\]</span></p>
<p>Fortunately, the observation matrix is less demanding, since the only
thing observed is the adjusted closing price. The following
specification assumes that the hidden state corresponding to the price,
affects directly the price that will be observed at the next step
without a scaling factor being required.</p>
<p><span class="math display">\[H = \begin{bmatrix}
1 &amp; 0 &amp; 0
\end{bmatrix}\]</span><mark></mark></p>
<p><mark></mark></p>
<p>The only thing that remains to be defined is the measurement noise
<span class="math inline">\(R\)</span>. In the present application, it
will be a scalar since the only measurement is the price. It will be set
to arbitrarily small to 0.1, since it is reasonable that the error in
the measurement should be negligible given the advances of
technology.</p>
<p>Note that measurement noise, along with the variance associated with
the acceleration, are assigned arbitrarily. While a low value for the
measurement noise reflects confidence in the reliability of the
data-source, a low variance for the acceleration implies confidence in
the predictions. With higher acceleration variance the model trusts
measurements more than predictions; with lower values for the
acceleration variance, the model trusts more the predictions.</p>
<p>It should be noted, as remarked above that the noise is not modeled
directly but through the variance and covariances implied by the
distributions.</p>
<p>Here the results!</p>

<p style="text-align:center"><mark></mark><img src="{{ '/assets/images/post_images/kalman_filter/amazon_stock_price_prediction_kalman_filter.png' | relative_url }}"
style="width:6.69305in;height:3.34653in" /><mark></mark><img
src="{{ '/assets/images/post_images/kalman_filter/amazon_stock_price_prediction_kalman_filter_velocities.png' | relative_url }}"
style="width:6.69305in;height:3.34653in" /><mark></mark></p>
<p><img src="{{ '/assets/images/post_images/kalman_filter/amazon_stock_price_prediction_kalman_filter_accelerations.png' | relative_url }}"
style="width:6.69305in;height:3.34653in" /></p>

<p>While it is possible to work a little bit more on the acceleration,
and it will be the undertake of a future article, we can say without fear:</p>
<h1 style="text-align:center"><strong>Goodbye ARIMA!</strong></h1>
<p>The present derivations are based on the techniques that I have
learned in my university courses, and they are no more than ordinary
Bayesian praxis. However, I would like to introduce a number of
references that might be useful.</p>
<p>Charles, S. A. (2017). Kalman Filter: A Bayesian Approach. Available
at:
https://www.bme.jhu.edu/ascharles/wp-content/uploads/2020/01/KalmanFilterBayesDerivation.pdf.</p>
<p>Jones, W. (2021). Forget ARIMA — Going Bayesian with Time Series
Analysis. <em>EMBECOSM</em>. Available at: <a
href="https://www.embecosm.com/2021/12/18/forget-arima-going-bayesian-with-time-series-analysis/"><u>https://www.embecosm.com/2021/12/18/forget-arima-going-bayesian-with-time-series-analysis/</u></a>.</p>
<p>Marwade, A. (2020). Kalman Filtering: An Intuitive Guide Based on
Bayesian Approach. <em>Medium.</em> Available at: <a
href="https://towardsdatascience.com/kalman-filtering-an-intuitive-guide-based-on-bayesian-approach-49c78b843ac7"><u>https://towardsdatascience.com/kalman-filtering-an-intuitive-guide-based-on-bayesian-approach-49c78b843ac7</u></a>.
<em><strong>This is a member-only story, refer to Charles’
article.</strong></em></p>
<p>Masnadi-Shirazi, H., Masnadi-Shirazi, A. and Dastgheib, M.-A., 2019.
A Step by Step Mathematical Derivation and Tutorial on Kalman Filters.
<em>arXiv</em>. Available at: <a
href="https://arxiv.org/abs/1910.03558"><u>https://arxiv.org/abs/1910.03558</u></a>.</p>
<p>I <em>would advise the reader to have a look at this link contained
in the Jones’ article:
https://web.mit.edu/kirtley/kirtley/binlustuff/literature/control/Kalman
filter.pdf. Moreover, the article provides an a link to a GitHub
repository with an implementation of the filter using Tensorflow,
compare</em> <em>it with my low-level implementation in numpy!
<br>
<br>
<h2 style="text-align:center"><a href="https://github.com/Gabriele-Donato/website-materials-/tree/BayesianTimeSeries">Here my code</a></h2></em></p>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Note that in this case variance of the state is not zero
because there is an intrinsic uncertainty in the prediction. Instead, at
time <span class="math inline">\(t\)</span> the the value of the hidden
state is fixed and therefore its variance is zero.<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The one used in the present article has been downloaded
from Yahoo Finance.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</body>
</html>
